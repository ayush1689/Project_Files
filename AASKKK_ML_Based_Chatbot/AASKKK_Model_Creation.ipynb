{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Kumar\n",
      "[nltk_data]     Ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Kumar\n",
      "[nltk_data]     Ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Kumar\n",
      "[nltk_data]     Ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import string\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from IPython.display import display, HTML\n",
    "import urllib\n",
    "import gzip\n",
    "import nltk\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk.stem\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from scipy.sparse import hstack\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "import pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanString(review):\n",
    "    # Setup the stopwords from the English dictionary\n",
    "    #This can be used to clean Entire Dataframe of Documents.\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    #Create an object for word lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    returnString = \"\"\n",
    "    #Tokenize the words from string to a list\n",
    "    sentence_token = tokenize.word_tokenize(review)\n",
    "    idx_list = []\n",
    "    for j in range(len(sentence_token)):\n",
    "        #For each sentence after tokenization, we take in the word for processing\n",
    "        single_sentence = tokenize.word_tokenize(sentence_token[j])\n",
    "        #We apply the word lemmetizer to lemmatize the word where we remove verb , adjactive inorder to provide the required word in the sentence.\n",
    "        single_sentence=[lemmatizer.lemmatize(t) for t in single_sentence]\n",
    "        # Filter stop words , make the all words in lower character and enumerate the sentence to remove special characters.\n",
    "        single_sentence=[word for word in single_sentence if word.lower() not in stopWords]\n",
    "        sentences_filtered = [(idx,lemmatizer.lemmatize(w.lower())) for idx,w in enumerate(single_sentence) \n",
    "                              if w.lower() not in stopWords and w.isalnum()]\n",
    "        idx_list.append([x[0] for x in sentences_filtered])\n",
    "        word_list = [x[1] for x in sentences_filtered]\n",
    "        returnString = returnString + ' '.join(word_list) + ' '\n",
    "    \n",
    "    return returnString\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_validation(model,train,y_train,validation,y_test):\n",
    " #APPLY CHARACTER VECTORIZER TRANSFORM ON VALIDATION DATA, 20000 dimensions of characters are created\n",
    "  #APPLY WORD VECTORIZER TRANSFORM ON VALIDATION DATA, 3000 dimensions of words are created\n",
    "     #LOWER THE DIMENSION USING SVD IN PRINCIPAL COMPONENT ANALYSIS TO VERY FEW 500 DIMENSIONS\n",
    "        #Normalize the incoming test stack and train data\n",
    "         #FIT THE MODEL and SAVE THE FITTED MODEL\n",
    "            # APPLY UNSUPERVISED METHOD PAIRWAISE COSINE DISTANCE BETWEEN THE INCOMING TEXT and DOCUMENT and FIND THE CLOSEST MATCH \n",
    "                #Unsupervised method ensures the words are matched with context of the document and closest match is choosen for prediction\n",
    "                \n",
    "    X_train_char=char_vectorizer.transform(train)\n",
    "    X_train_word=word_vectorizer.transform(train)\n",
    "    train_stack = hstack([X_train_char,X_train_word])\n",
    "    \n",
    "    X_test_char = char_vectorizer.transform(validation)\n",
    "    X_test_word = word_vectorizer.transform(validation)\n",
    "    test_stack = hstack([X_test_char,X_test_word])\n",
    "    \n",
    "    svd_model=pickle.load( open( file1+\"_SVD_preprocessing.sav\", \"rb\" ) )\n",
    "    train_stack=svd_model.transform(train_stack)\n",
    "    test_stack=svd_model.transform(test_stack)\n",
    "    \n",
    "    train_features2 = Normalizer(copy=False).fit_transform(train_stack)\n",
    "    test_stack = Normalizer(copy=False).transform(test_stack)\n",
    "    \n",
    "    \n",
    "    fitted_model=model.fit(train_features2, y_train)\n",
    "    filename = file1+\"MLP\"+'_model.sav'\n",
    "    pickle.dump(fitted_model, open(filename, 'wb'))\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    best_thread = pairwise_distances_argmin(\n",
    "            X=test_stack,\n",
    "            Y=train_features2,\n",
    "            metric='cosine'\n",
    "        )\n",
    "    \n",
    "    \n",
    "  \n",
    "    \n",
    "    y_pred_class1 = model.predict(train_features2[best_thread])\n",
    "    y_pred_class2 = model.predict(test_stack)\n",
    "    \n",
    "   \n",
    "   \n",
    "   \n",
    "    print(\"Training Accuracy\")\n",
    "    print(model.score(train_features2,y_train))\n",
    "    print(\"Testing Accuracy1\")\n",
    "    print(model.score(test_stack,y_test))\n",
    "    print(\"Testing Accuracy2\")\n",
    "    print(model.score(train_features1[best_thread],y_test))\n",
    "    return(y_pred_class1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_feature_name:20000\n",
      "word_feature_name:2587\n",
      "Explained Variance:1.0\n"
     ]
    }
   ],
   "source": [
    "file1 = \"Combined_Corpus\"\n",
    "stats=pd.read_csv(file1+\".csv\")\n",
    "labelEncoder = LabelEncoder()\n",
    "response = dict(zip(stats.Topic_Categories, stats.Response))\n",
    "\n",
    "\n",
    "stats[\"Topic_Categories\"] = labelEncoder.fit_transform(stats[\"Topic_Categories\"])\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(file1+'_labelEncoder.sav', 'wb') as file:\n",
    "    pickle.dump(labelEncoder, file, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    \n",
    "with open(file1+'_ResponseDictionary.sav', 'wb') as file:\n",
    "    pickle.dump(response, file, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    \n",
    "stats = stats.rename(columns={'Topic_Categories': 'Category', 'Question': 'Text'})\n",
    "data_df=stats.sample(frac=1).reset_index(drop=True)\n",
    "data_df[\"Text\"]=data_df.Text.apply(cleanString)\n",
    "y=data_df[\"Category\"]\n",
    "X=data_df[\"Text\"]\n",
    "\n",
    "with open(file1+'_Data_df.sav', 'wb') as file:\n",
    "    pickle.dump(data_df, file, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.01, random_state=42)\n",
    "\n",
    "\n",
    "def build_tokenizer(doc):\n",
    "    token_pattern=r\"(?u)\\b\\w+\"\n",
    "    token_pattern = re.compile(token_pattern)\n",
    "    return token_pattern.findall(doc)\n",
    "\n",
    "\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc:(english_stemmer.stem(word) for word in analyzer(doc))\n",
    "    \n",
    "posts_root1=[]\n",
    "\n",
    "for post in X_train:\n",
    "    a=\" \".join([english_stemmer.stem(word) for word in build_tokenizer(post)])\n",
    "    posts_root1.append(a)\n",
    "\n",
    "\n",
    "\n",
    "word_vectorizer = StemmedTfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    stop_words = 'english',\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1,7),\n",
    "    norm='l2',\n",
    "    dtype=np.float32,\n",
    "    max_features=3000,\n",
    "    \n",
    ")\n",
    "# Character Stemmer\n",
    "char_vectorizer = StemmedTfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 10),\n",
    "    dtype=np.float32,\n",
    "    norm='l2',\n",
    "    max_features=20000,\n",
    "   \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "word_vectorizer.fit(posts_root1)\n",
    "char_vectorizer.fit(posts_root1)\n",
    "\n",
    "\n",
    "train_word_features = word_vectorizer.transform(posts_root1)\n",
    "train_char_features = char_vectorizer.transform(posts_root1)\n",
    "\n",
    "\n",
    "\n",
    "train_features = hstack([\n",
    "    train_char_features,\n",
    "    train_word_features])\n",
    "tsvd=TruncatedSVD(1000)\n",
    "\n",
    "train_features1 = tsvd.fit_transform(train_features)\n",
    "filename = file1+'train_features1_preprocessing.sav'\n",
    "pickle.dump(train_features1, open(filename, 'wb'))\n",
    "filename = file1+'_word_preprocessing.sav'\n",
    "pickle.dump(word_vectorizer, open(filename, 'wb'))\n",
    "filename = file1+'_char_preprocessing.sav'\n",
    "pickle.dump(char_vectorizer, open(filename, 'wb'))\n",
    "filename = file1+'_SVD_preprocessing.sav'\n",
    "pickle.dump(tsvd, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "print(\"char_feature_name:%s\" % len(char_vectorizer.get_feature_names()))\n",
    "print(\"word_feature_name:%s\" % len(word_vectorizer.get_feature_names()))\n",
    "print(\"Explained Variance:%s\" % tsvd.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy\n",
      "0.9924698795180723\n",
      "Testing Accuracy1\n",
      "0.9900990099009901\n",
      "Testing Accuracy2\n",
      "0.9504950495049505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([72, 72, 36, 55,  2, 91, 72, 18, 42,  0, 69, 64, 82, 73, 80,  4,  4,\n",
       "       47, 80, 47,  9, 36, 70, 58, 47, 90, 75, 54, 74, 73, 81, 65, 55, 64,\n",
       "       54, 49, 80, 54, 53, 80,  3, 50, 56, 64, 40, 43, 52, 60, 40, 55, 66,\n",
       "       40, 69, 30, 82, 83, 39, 92, 65, 27, 67, 40, 94,  1, 69, 62, 30, 79,\n",
       "       60, 80, 73, 85, 47, 30,  3, 57,  8, 81, 88, 60, 63, 93, 67, 69, 55,\n",
       "       70, 30, 49, 51, 62, 93, 42, 65, 58, 61, 42, 43, 93, 56, 58, 85, 40,\n",
       "       54, 36, 55, 55,  8, 55, 95, 49, 75, 27, 70, 74, 56, 81, 75, 36, 65,\n",
       "       93, 22, 81, 90, 20, 93, 43, 91, 56, 73, 89, 56, 74, 92, 54, 93, 79,\n",
       "       72, 57, 89, 12, 65, 81,  2, 41, 87, 28,  4, 69, 30, 35, 54, 86, 65,\n",
       "       45, 59, 65, 73, 56, 94, 51, 54, 61, 41, 80, 58, 49, 56, 55, 36, 61,\n",
       "       60, 80, 56, 60, 95, 72, 60, 34, 95, 64, 49, 40, 55, 49, 51, 73, 77,\n",
       "       53, 95, 89, 90, 49,  4, 61, 40, 89, 55, 38, 29, 60, 74, 94])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.columns=[\"Topic_Code\",\"Topic_Categories\",\"Question\",\"Response\"]\n",
    "data_df=stats.sample(frac=1).reset_index(drop=True)\n",
    "data_df[\"Question\"]=data_df.Question.apply(cleanString)\n",
    "\n",
    "\n",
    "y=data_df[\"Topic_Categories\"]\n",
    "X=data_df[\"Question\"]\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X,y,test_size=0.3, random_state=42)\n",
    "mlp=MLPClassifier(activation='relu', alpha=0.001, batch_size='auto', beta_1=0.75,\n",
    "              beta_2=0.8, early_stopping=False, epsilon=1e-08,\n",
    "              hidden_layer_sizes=(300, 300), learning_rate='constant',\n",
    "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
    "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
    "              power_t=0.5, random_state=200, shuffle=True, solver='adam',\n",
    "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "              warm_start=False)\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "\n",
    "\n",
    "\n",
    "train_validation(mlp,X_train,y_train,X_test1,y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
